import uuid
import random
from datetime import datetime, timedelta
from polars import count, first, lit
from pyspark.sql.window import Window
from pyspark.sql.types import StringType, FloatType, TimestampType
from pyspark.sql.functions import expr, rand, udf, row_number, floor, when, col, explode, array_repeat



def sql_query(project_id:str, dataset_id:str, row_limit:int) -> dict:
    """
        Generates SQL query strings for retrieving customer and product data from specified BigQuery tables.

        Args:
            project_id (str): The Google Cloud project ID.
            dataset_id (str): The BigQuery dataset ID.
            row_limit (int): The maximum number of rows to retrieve for the customers query.

        Returns:
            dict: A dictionary containing two SQL query strings:
                - "tb_customers": SQL query to select associate, card, and address IDs from customers, cards, and address tables,
                limited to 70% of the specified row_limit, with randomized ordering.
                - "tb_products": SQL query to select inventory and product details from products and inventory tables.
    """
    return {
        "tb_customers": f"""
            SELECT
                TBCU.associate_id,
                TBCA.card_id,
                TBAD.address_id,
                TBIN.region
            FROM
                `{project_id}.{dataset_id}.tb_customers` AS TBCU
            INNER JOIN
                `{project_id}.{dataset_id}.tb_cards` AS TBCA
            ON
                TBCU.associate_id = TBCA.fk_associate_id
            INNER JOIN
                `{project_id}.{dataset_id}.tb_address` AS TBAD
            ON
                TBCU.associate_id = TBAD.fk_associate_id
            ORDER BY
                RAND()
            LIMIT {int(row_limit * (70 / 100))};
        """,
        "tb_products": f"""
            SELECT
                TBIN.inventory_id,
                TBPR.product_id,
                TBPR.price,
                TBIN.location,
                TBIN.region
            FROM
                `{project_id}.{dataset_id}.tb_products` AS TBPR
            INNER JOIN
                `{project_id}.{dataset_id}.tb_inventory` AS TBIN
            ON
                TBPR.product_id = TBIN.product_id;
        """
    }


def _generate_order_id() -> str:
    """
        Generates a unique order ID string.

        Returns:
            str: A unique order ID in the format 'SALE##<UUID>'.
    """
    return f"SALE##{str(uuid.uuid4())}"


def _generate_random_date() -> datetime:
    """
        Generates a random datetime within the past 180 days from the current date.

        Returns:
            datetime: A randomly generated datetime object between now and 180 days ago.
    """
    days_ago = random.randint(0, 180)
    return datetime.now() - timedelta(days=days_ago)


def _generate_peak_season_multiplier() -> float:
    """
        Generates a random peak season multiplier.

        Returns:
            float: A random float value between 1.2 and 2.5 (inclusive), rounded to one decimal place.
    """
    return round(random.uniform(1.2, 2.5), 1)


def _udf_functions() -> list:
    """
        Registers UDFs (User Defined Functions) for generating random dates, peak season multipliers, and order IDs.

        Returns:
            tuple: A tuple containing three UDFs:
                - generate_random_date_udf: UDF for generating random dates.
                - generate_peak_season_multiplier_udf: UDF for generating peak season multipliers.
                - generate_order_id_udf: UDF for generating order IDs.
    """
    generate_random_date_udf = udf(_generate_random_date, TimestampType())
    generate_peak_season_multiplier_udf = udf(_generate_peak_season_multiplier, FloatType())
    generate_order_id_udf = udf(_generate_order_id, StringType())

    return generate_random_date_udf, generate_peak_season_multiplier_udf, generate_order_id_udf

#TODO: I must test this function
def generate_fake_purchases(df_customers, df_products, VAR_NUM_PURCHASES):
    """
    Generates a DataFrame of fake purchase records by randomly pairing customers and products from the same region.

    Args:
        spark (pyspark.sql.SparkSession): The active Spark session.
        df_customers (pyspark.sql.DataFrame): DataFrame containing customer data.
        df_products (pyspark.sql.DataFrame): DataFrame containing product data.
        VAR_NUM_PURCHASES (int): The number of fake purchase records to generate.

    Returns:
        pyspark.sql.DataFrame: A DataFrame containing VAR_NUM_PURCHASES rows, each representing a fake purchase with
        customer and product information from the same region.

    Notes:
        - Each purchase is generated by randomly selecting a customer and a product from the same region.
        - The resulting DataFrame does not include the index columns used for joining.
    """
    w_cust = Window.partitionBy("region").orderBy(rand())
    w_prod = Window.partitionBy("region").orderBy(rand())

    df_customers_indexed = df_customers.withColumn("cust_idx", row_number().over(w_cust) - 1)
    df_products_indexed = df_products.withColumn("prod_idx", row_number().over(w_prod) - 1)


    region_stats = df_customers_indexed.groupBy("region").agg(
        count("*").alias("total_customers"),
        first(lit(0)).alias("dummy"))

    region_stats = region_stats.join(
        df_products_indexed.groupBy("region").agg(count("*").alias("total_products")),
        on="region",
        how="left"
    ).fillna(0)


    purchases_per_region = df_customers_indexed.groupBy("region").count()
    purchases_per_region = purchases_per_region.withColumn(
        "purchase_count",
        round(col("count") / sum("count").over(Window.orderBy()) * VAR_NUM_PURCHASES)
    ).drop("count")


    total_diff = VAR_NUM_PURCHASES - purchases_per_region.agg(sum("purchase_count")).first()[0]
    purchases_per_region = purchases_per_region.withColumn(
        "purchase_count",
        when(rand() < total_diff/VAR_NUM_PURCHASES, col("purchase_count") + 1).otherwise(col("purchase_count"))
    )


    df_base = purchases_per_region.withColumn(
        "id",
        explode(array_repeat(lit(1), col("purchase_count")))
    ).drop("id", "purchase_count")

    df_base = df_base.join(region_stats, on="region", how="left")

    df_base = df_base.withColumn(
        "cust_idx",
        floor(rand() * col("total_customers"))
    ).withColumn(
        "prod_idx",
        floor(rand() * col("total_products"))
    ).drop("total_customers", "total_products", "dummy")

    return df_base \
        .join(df_customers_indexed, on=["region", "cust_idx"], how="inner") \
        .join(df_products_indexed, on=["region", "prod_idx"], how="inner") \
        .drop("cust_idx", "prod_idx")


def generate_fake_orders(df_base):
    """
        Generates a DataFrame of fake order data by augmenting the input DataFrame with synthetic order-related columns.

        Args:
            df_base (pyspark.sql.DataFrame): The base DataFrame containing at least the columns:
                'associate_id', 'card_id', 'address_id', 'product_id', 'inventory_id', 'location', 'region', and 'price'.

        Returns:
            pyspark.sql.DataFrame: A new DataFrame with additional columns for simulated order data, including:
                - purchase_id: Unique identifier for each order.
                - quantity: Randomly generated order quantity.
                - price: Original price from the base DataFrame.
                - discount_applied: Random discount applied to the order.
                - final_price: Price after discount and quantity calculation.
                - order_status: Randomly assigned order status ('completed', 'processing', or 'cancelled').
                - purchase_date: Randomly generated purchase date.
                - normal_processing_time: Random normal processing time.
                - expedited_processing_time: Random expedited processing time.
                - peak_season_multiplier: Random multiplier for peak season.
                - Other identifying columns from the base DataFrame.

        Note:
            This function relies on internal UDFs for generating random dates, order IDs, and peak season multipliers.
    """

    generate_random_date_udf, generate_peak_season_multiplier_udf, generate_order_id_udf = _udf_functions()

    df_base = \
        df_base.withColumn("purchase_id", generate_order_id_udf()) \
            .withColumn("purchase_date", generate_random_date_udf()) \
            .withColumn("quantity", expr("CAST(FLOOR(RAND() * 10 + 1) AS INT)")) \
            .withColumn("normal_processing_time", expr("CAST(FLOOR(RAND() * 10 + 1) AS INT)")) \
            .withColumn("expedited_processing_time", expr("CAST(FLOOR(RAND() * 5 + 1) AS INT)")) \
            .withColumn("peak_season_multiplier", generate_peak_season_multiplier_udf()) \
            .withColumn(
            "order_status",
                when(rand() < 0.80, "completed")
                .when(rand() < 0.90, "processing")
                .otherwise("cancelled")) \
            .withColumn("discount_applied", expr("ROUND(RAND() * 20, 2)")) \
            .withColumn("final_price", expr("ROUND((price * quantity) - discount_applied, 2)"))


    df_base = \
        df_base.select(
            "purchase_id",
            "associate_id",
            "card_id",
            "address_id",
            "product_id",
            "inventory_id",
            "location",
            "region",
            "quantity",
            "price",
            "discount_applied",
            "final_price",
            "order_status",
            "purchase_date",
            "normal_processing_time",
            "expedited_processing_time",
            "peak_season_multiplier",
        )

    return df_base